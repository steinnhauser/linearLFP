""" Functionality implemented to read in a ECS potential signal from
an outside source and apply the kernel to a firing rate signal """

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import numpy as np
import json
import h5py

import utils.plotter as pf
import utils.misc as mf

jfile = open("./init/setup.json", "r", encoding="utf-8")
jdict = json.load(jfile)
jfile.close()

args = vars(mf.argument_parser())
save_lfp_signals = args["save_lfp_signals"]
nidx = int(jdict['nidx']*args["n_idx"]) #outside-stimulating synapses per neuron
custom_group_index = args["custom_group_index"]

# Parse all the information about the populations and their sizes
populations = {
    pop : int(jdict['population_sizes'][n] * args["scale_factor"])
    for n, pop in enumerate(jdict['population_names'])
}

""" Read in the kernels generated by the previous script. """
datadir = "./data/"
pops = ["E:E", "E:I", "I:E", "I:I"]

names = ["kernel_" + i + ".h5" for i in pops]

print("Simulating kernel method...")

# read in the data. Keep it in a good format for the draw_lineplot function.
kernel_dict = {
    p : [[] for channels in range(13)] for p in pops
}

for pop, name in zip(pops, names):
    hf = h5py.File(datadir + name, 'r')
    for k in range(13):
        # extract and normalize
        kernel_dict[pop][k] = np.array(hf.get(f"ch{k+1}"))
    hf.close()

""" --------------------------------------------------- """
""" Read in the spike times of the generated simulation """
# Read in the 'savedSpikes.h5' file generated by 'example_network'.
READPATH = "./true_sim/"
sph5 = h5py.File(READPATH + "savedSpikes.h5", "r")

""" Generate a gid_spikes dictionary. Important here to note that each
population is saved separately, such that the HDF .get().get() is required
for the two level system. This function generates a list of lists for each
neuron in the population in the following fashion: [[350, 507],[200, 700],...]
"""
gid_spikes = { # generate a dictionary
    pop_name : [
        [spt] for spt in sph5.get(f'{pop_name}').get('spikeTimes')
    ] for pop_name in populations.keys()
}
sph5.close()

""" --------------------------------------------------- """
""" Establish a time axis for all the items """
numpoints_kernels = int(jdict['stim_sim_len']/(2**jdict['dt'])) + 1
numpoints_sims = int(jdict['tstop']/(2**jdict['dt'])) + 1
time_axis_kernels = np.linspace(0, jdict['stim_sim_len'], numpoints_kernels)
time_axis_sims = np.linspace(0, jdict['tstop'], numpoints_sims)

""" Generate a spike train for all the populations and apply the kernels """
""" Begin by filtering out only kernels that are between 100 and 150ms """
beginidx = np.where(time_axis_kernels >= (jdict["stim_sim_time"]-100))[0][0] # centralize 100ms
endidx = np.where(time_axis_kernels >= (jdict["stim_sim_time"]+100))[0][0] # try x[150>x>50]
for p in pops:
    for ch in range(13):
        resting_val = kernel_dict[p][ch][beginidx] # assume that resting is at 50ms
        kernel_dict[p][ch] = [i-resting_val # subtract DC component and contract
            for i in kernel_dict[p][ch][beginidx:endidx]]

time_axis_kernels = time_axis_kernels[beginidx:endidx]

""" Take the times from self.gid_spikes and transform to a
list of 0s & 1s in a 'delta function' format """
FR_dict = {
    "E" : np.zeros(numpoints_sims),
    "I" : np.zeros(numpoints_sims)
}

for pop_name, pop_size in populations.items():
    delta_fr_train, _ = np.histogram(gid_spikes[pop_name],
            bins=numpoints_sims,
            range=(0, jdict["tstop"]))

    FR_dict[pop_name] += delta_fr_train


""" Sum up all the E:E, E:I, I:E, I:I contributions into one ECS signal """
final_vals = [np.zeros(numpoints_sims) for channels in range(13)]

for p in pops:
    for ch in range(13):
        # convolve each of the populations E:E and E:I on the E firing rates.
        final_vals[ch] += np.convolve(
            FR_dict[p[0]],
            kernel_dict[p][ch],
            'same'
        )


if save_lfp_signals:
    # if we're in the lfp signal per nidx scheme, append to a data analysis frame
    dir = "./analysis/prepdata/"
    nm = "lfp_signal_kernel.h5"
    hdf_file = h5py.File(dir+nm, 'a') # append to already existing list

    if custom_group_index == -1:
        grp = hdf_file.create_group(f"nidx_{nidx}")
    else: # if the study does not change nidx value, need to save as custom.
        grp = hdf_file.create_group(f'{custom_group_index}')

    for k in range(13):
        key = f"ch{k+1}"
        grp.create_dataset(
            key, data=np.array(final_vals[k])
            )
    hdf_file.close()


# simply overwrite and save if not the case anyway
dir = "./data/"
nm = "kernel_signal_lfp.h5"
hdf_file = h5py.File(dir+nm,'w')
for k in range(13):
    key = f"ch{k+1}"
    hdf_file.create_dataset(
        key, data=np.array(final_vals[k])
        )
hdf_file.close()
